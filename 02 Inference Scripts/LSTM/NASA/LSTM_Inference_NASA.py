# -*- coding: utf-8 -*-
"""LSTM Inference NASA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n_wvay3_Qvk8lXGzLeT2kDO0ONc5j_23

# LSTM Encoder-Decoder Inference
"""

# Setting up argument parser
import argparse
parser = argparse.ArgumentParser()
parser.add_argument('-p','--print',action='store_true',
                    help='Print to 16x2 LCD Display')
args = parser.parse_args()

"""## Packages and Functions for Evaluating Performance"""

# For timing
from timeit import default_timer as timer
import torch

def print_inference_time(start: float,
                     end: float,
                     device: torch.device = None,
                     print_time = True):
  """
  Prints difference between start and end time
  """

  total_time = end - start
  if print_time:
    print(f"Inference time on {device}: {total_time:.3f} seconds")
  
  return total_time

# For memory tracing
import tracemalloc

def memory_stats(snapshot, key_type='lineno', print_mem=True):
  '''
  Compute memory usage from a tracemalloc snapshot.
  Results are in KiB (Kibibytes).
  '''
  snapshot = snapshot.filter_traces((
      tracemalloc.Filter(False, "<frozen importlib._bootstrap>"),
      tracemalloc.Filter(False, "<unknown>"),
  ))
  usage_stat = snapshot.statistics(key_type)

  total = sum(stat.size for stat in usage_stat)
  total = total / 1024

  if print_mem:
    print(f"Allocated memory: {total:.2f} KiB")

  return total

"""## Start Memory profiling"""

tracemalloc.start()

"""## Start of inference

### Import packages
"""

import numpy as np
import random
import math
import os
import torch
import torch.nn as nn
import torch.nn.functional as F

"""### Device agnostic code"""

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
rul_preds_list = {}

"""### LSTM Decoder-Encoder Architecture"""

class LSTM_ENCODER(nn.Module):
    def __init__(self, input_size, hidden_size, device, num_layers = 1):
        super(LSTM_ENCODER, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.device = device

        # define LSTM Layer
        self.lstm = nn.LSTM(input_size =  input_size, hidden_size = hidden_size,
                            num_layers = num_layers)
    
    def forward(self, x_input):
        # x_input -- input of shape(sequence_length, # in batch, input_size)
        lstm_out, self.hidden = self.lstm(x_input.view(x_input.shape[0], x_input.shape[1], self.input_size))

        return lstm_out, self.hidden
    def init_hidden(self, batch_size):
        # batch_size = x_input.shape[1]
        return (torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device),
                torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device))
        
class LSTM_DECODER(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers = 1):
        super(LSTM_DECODER, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,
                            num_layers = num_layers)
        self.linear = nn.Linear(hidden_size, input_size)

    def forward(self, x_input, encoder_hidden_states):
        # x_input  should be 2D(batch_size, input_size)
        # encoder_hidden_states hidden states
        lstm_out, self.hidden = self.lstm(x_input.unsqueeze(0), encoder_hidden_states)
        output = self.linear(lstm_out.squeeze(0))

        return output, self.hidden

class LSTMSequential(nn.Module):
    ''' train and LSTM encoder-decoder and make predictions '''
    def __init__(self, input_size, hidden_size):
        super(LSTMSequential, self).__init__()

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.device = device

        self.encoder = LSTM_ENCODER(input_size = input_size, device=self.device, hidden_size = hidden_size)
        self.decoder = LSTM_DECODER(input_size = input_size, hidden_size = hidden_size)

    def train_model(self,
                    input_tensor,
                    target_tensor,
                    n_epochs,
                    target_len,
                    batch_size,
                    device,
                    seed,
                    training_prediction = 'recursive',
                    teacher_forcing_ratio = 0.5,
                    learning_rate = 0.01, dynamic_tf = False):
        '''
        
        train lstm encoder-decoder
        
        '''
        # setup seed
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)

        # initialize array of losses
        losses = np.full(n_epochs, np.nan)

        optimizer = optim.Adam(self.parameters(), lr = learning_rate)
        criterion = nn.MSELoss()

        # calculate number of batch iterations
        n_batches = int(input_tensor.shape[1] / batch_size)

        with trange(n_epochs) as tr:
            for it in tr:

                batch_loss = 0.
                batch_loss_tf = 0.
                batch_loss_no_tf = 0.
                num_tf = 0
                num_no_tf = 0

                for b in range(n_batches):
                    # select data
                    input_batch = input_tensor[:, b: b + batch_size, :]
                    target_batch = target_tensor[:, b: b + batch_size, :]

                    # outputs tensor
                    outputs = torch.zeros(target_len, batch_size, input_batch.shape[2], device=device)

                    # initialize hidden_state
                    encoder_hidden = self.encoder.init_hidden(batch_size)

                    # zero the gradient
                    optimizer.zero_grad()

                    # encoder outputs
                    encoder_output, encoder_hidden = self.encoder(input_batch)

                    # decoder with teacher forcing
                    decoder_input = input_batch[-1, :, :]   # shape: (batch_size, input_size)
                    decoder_hidden = encoder_hidden

                    if training_prediction == 'recursive':
                        # predict recursively
                        for t in range(target_len):
                            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)
                            outputs[t] = decoder_output
                            decoder_input = decoder_output
                    
                    if training_prediction == 'teacher_forcing':
                        # use teacher forcing
                        if random.random() < teacher_forcing_ratio:
                            for t in range(target_len):
                                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)
                                outputs[t] = decoder_output
                                decoder_input = target_batch[t, :, :]

                        # predict recursively
                        else:
                            for t in range(target_len):
                                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)
                                outputs[t] = decoder_output
                                decoder_input = decoder_output
                    
                    if training_prediction == 'mixed_teacher_forcing':
                        # predict using mixed teacher forcing
                        for t in range(target_len):
                            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)
                            outputs[t] = decoder_output

                            # predict with teacher forcing
                            if random.random() < teacher_forcing_ratio:
                                decoder_input = target_batch[t, :, :]

                            #predict recursively
                            else:
                                decoder_input = decoder_output

                    # compute the loss
                    loss = criterion(outputs, target_batch)
                    batch_loss += loss.item()

                    # backpropagation
                    loss.backward()
                    optimizer.step()

                # loss for epoch
                batch_loss /= n_batches
                losses[it] = batch_loss

                # dynamic teacher forcing
                if dynamic_tf and teacher_forcing_ratio > 0:
                    teacher_forcing_ratio = teacher_forcing_ratio - 0.02

                # progress bar
                tr.set_postfix(loss="{0:.3f}".format(batch_loss))
            
        return losses

    def predict(self, input_tensor, target_len):
        '''
        
        make test predictions

        '''

        # encode input_tensor
        input_tensor = input_tensor.unsqueeze(1)       # add in batch size of 1
        encoder_output, encoder_hidden = self.encoder(input_tensor)

        # initialize tensor for predictions
        outputs = torch.zeros(target_len, input_tensor.shape[2])

        # decode input_tensor
        decoder_input = input_tensor[-1, :, :]
        decoder_hidden = encoder_hidden
        
        for t in range(target_len):
            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)
            outputs[t] = decoder_output.squeeze(0)
            decoder_input = decoder_output

        np_outputs = outputs.detach().numpy()

        return np_outputs

"""### Inference"""

# setup seed
seed = 2
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)

"""Load variables and models"""

# Battery list and load battery
Battery_list = ['B0005', 'B0006', 'B0007', 'B0018']
Battery = np.load('NASA.npy', allow_pickle=True)
Battery = Battery.item()

# Parameters
Rated_Capacity = {}
Rated_Capacity['B0005'] = 2.0143
Rated_Capacity['B0006'] = 1.9857
Rated_Capacity['B0007'] = 2.0143
Rated_Capacity['B0018'] = 2.0143

window_size = 16
hidden_size = 64
input_window = 16
output_window = 16
stride = 1

# DETREND setting and load scaler
from sklearn.preprocessing import StandardScaler
import joblib
from math import sqrt

DETREND = True
scaler = joblib.load('scaler_LSTM_NASA.save') 

# Load test tensors
x_train, x_test, X_train, Y_train, X_test, Y_test = torch.load('test_tensors_LSTM_NASA.pt', map_location=torch.device(device))

# Load model
model = {}
for leave_out in Battery_list:
  model[leave_out] = LSTMSequential(input_size = X_train[leave_out][leave_out].shape[2], hidden_size = hidden_size).to(device)
  model[leave_out].load_state_dict(torch.load(f'checkpoints/LSTM_NASA_{leave_out}.pth', map_location=device))

# End memory snapshot for setup
memsnap_end_setup = tracemalloc.take_snapshot()
memuse_end_setup = memory_stats(memsnap_end_setup, print_mem=False)
# End capture memory stats before inference
tracemalloc.stop()

"""### Multi-step ahead"""

def retrend_function(Batt_Val, denormalize, name = 'B0005', split_ratio = '0.6'):
    final = []
    idx_lastpoint = int(len(Batt_Val[name][1])*0.6) - 1
    past_val = Batt_Val[name][1][idx_lastpoint]
    for i in range(len(denormalize)):
        pres_val = past_val + denormalize[i]
        final.append(pres_val)
        past_val = pres_val
    return final

# Compute for accuracy
def accuracy(y_test, y_pred, numpy=True):
  if numpy:
    error = np.abs(y_pred-y_test)/y_test
    acc = np.ones_like(error) - error
    acc = np.sum(acc)/len(y_pred)
  else:
    error = torch.abs(y_pred-y_test)/y_test
    acc = torch.ones_like(error) - error
    acc = torch.sum(acc)/len(y_pred)
  
  return float(acc)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import matplotlib as mpl
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

final_output={}
MSP_LSTM_NASA = {}

plt.figure(figsize=(40,7))
plt.suptitle(f'LSTM (NASA) Multi-step Ahead',fontsize=17, weight='bold')

for leave_out, idx in zip(Battery_list, range(len(Battery_list)) ):
  # Start timer and tracemalloc
  start_time = timer()
  tracemalloc.start()

  # preliminaries
  final_output[leave_out] = {}
  model[leave_out].eval()
  output_len = len(x_test[leave_out][leave_out])
  outputs = np.empty((0,output_len))

  # Do the predictions
  with torch.no_grad():
      xl = len(x_train[leave_out][leave_out])
      train_period = input_window
      dict_res = x_train[leave_out][leave_out]
      x = dict_res[xl-train_period: xl]   
       
      outputs_len = output_len + (output_window-(output_len%output_window))
      
      while len(outputs) < outputs_len:
          Y_test_pred = model[leave_out].predict(torch.from_numpy(x).type(torch.Tensor).to(device), target_len = output_window)
          outputs = np.concatenate((outputs, Y_test_pred), axis=None)
          x = x[output_window:output_window+input_window]
          x = np.append(x, [Y_test_pred])
          x = x.reshape(-1,1)
  new_outputs = outputs[0:output_len+1]

  # Detrend
  if DETREND:
    denormalize = scaler[leave_out].inverse_transform(np.array(new_outputs).reshape(-1, 1))
    final_output[leave_out][leave_out] = retrend_function(Batt_Val=Battery, denormalize=denormalize, name=leave_out)
    final_output[leave_out][leave_out] = np.concatenate(final_output[leave_out][leave_out],axis=0)
  else:
    final_output[leave_out][leave_out] = new_outputs
  
  # End timer and capture memory stats after inference
  end_time = timer()
  memsnap_post_onestep = tracemalloc.take_snapshot()
  tracemalloc.stop()

  # Computing memory usage
  memuse_post_onestep = memory_stats(memsnap_post_onestep, print_mem=False)
  memuse_total = memuse_post_onestep + memuse_end_setup

  # Metrics
  x, y = Battery[leave_out]
  y_true = y[-len(final_output[leave_out][leave_out]):]
  y_preds = final_output[leave_out][leave_out]  
  acc = accuracy(np.array(y_true),np.array(y_preds))*100
  mae = mean_absolute_error(np.array(y_true),np.array(y_preds))
  rmse = sqrt(mean_squared_error(np.array(y_true),np.array(y_preds)))
  print(f"Long Short-Term Memory (window_size={window_size}, hidden_dim={hidden_size})\nMulti-step ahead Prediction")
  print("="*60)
  print(f"Metrics for Model Prediction on {leave_out}")
  total_time = print_inference_time(start_time, end_time, device)
  print(f"Mem alloc for inference: {memuse_post_onestep} KiB \t Total including init: {memuse_total:.3f}KiB")
  print(f"MAE: {mae:.4f} | RMSE: {rmse:.4f} | Accuracy: {acc:.4f}%")

  # Computing RUL Error
  Threshold = 0.7 * Rated_Capacity[leave_out]
  idx_true = (torch.tensor(y_true)<Threshold).nonzero()        # search idx less than threshold
  RUL_true = idx_true[0][0]                        # first entry is true RUL

  idx_preds = (torch.tensor(y_preds)<Threshold).nonzero()          # search idx less than threshold
  RUL_preds = idx_preds[0][0]                         # first entry is pred RUL
  RUL_error = RUL_true - RUL_preds                 # if positive value, early RUL; 
                                                   # negative value, late RUL
  RUL_relative_error = RUL_error / RUL_true
  rul_preds_list[leave_out] = RUL_preds
  print(f"RUL True: {RUL_true} | RUL Predicted: {RUL_preds} | RUL Error: {RUL_error}\n{'-'*60}")

  # Plot
  plt.subplot(1,4,idx+1)
  x, y = Battery[leave_out]

  plt.title(f'Prediction for Battery {leave_out}')
  plt.ylabel('Capacity (Ah)')
  plt.xlabel('Discharge Cycles')
  plt.grid(True)
  plt.plot(x[:-len(final_output[leave_out][leave_out])], y[:-len(final_output[leave_out][leave_out])], "k-")
  plt.plot(x[-len(final_output[leave_out][leave_out]):], y[-len(final_output[leave_out][leave_out]):], "k--", label='Ground Truth')
  plt.plot(x[-len(final_output[leave_out][leave_out]):], final_output[leave_out][leave_out], "b-", label='Prediction')
  plt.legend()

  MSP_LSTM_NASA[leave_out] = {
      "model_name": leave_out,
      "mem_usage": memuse_total,
      "exec_time": total_time,
      "acc": acc,
      "mae": mae,
      "rmse": rmse,
      "rul_error": int(RUL_error),
      "RUL_relative_error": float(RUL_relative_error)
  }
  
plt.savefig(f'Multi-Step_LSTM_NASA.pdf')

import pandas as pd
# Convert to PD DataFrame
MSP_results = pd.DataFrame([MSP_LSTM_NASA['B0005'],
                            MSP_LSTM_NASA['B0006'],
                            MSP_LSTM_NASA['B0007'],
                            MSP_LSTM_NASA['B0018']
                            ])
# Saving results
MSP_results.to_pickle("MSP_ELM_NASA.pkl")

# Print to 16x2 LCD
if args.print:
    # LCD Stuff
    from Adafruit_CharLCD import Adafruit_CharLCD
    from time import sleep

    # Initialize LCD and specify pins
    lcd = Adafruit_CharLCD(rs=26, en=19,
                           d4=13, d5=6, d6=5, d7=21,
                           cols=16, lines=2)
    # Clear the LCD
    lcd.clear()
    for leave_out in Battery_list:
        # Display message
        lcd.clear()
        lcd.message(f'Cycles left for\n{leave_out}: {rul_preds_list[leave_out]}')
        sleep(5)
